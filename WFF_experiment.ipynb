{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7ffce4",
   "metadata": {},
   "source": [
    "Author: Steve Bischoff\n",
    "\n",
    "This notebook contains code defining the first-order language, randomly constructing an interpretation, building a training dataset of true well-formed formulae, training a decoder-only transformer on next-word prediction, and tracking ambiguity statistics throughout training. The results of only one round of the experiment are printed here, though the code can easily be modified to run more rounds.\n",
    "\n",
    "The code for the model itself is adapted with minor changes from: https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb6363",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    " 1. [Language Preliminaries](#language)\n",
    " 2. [WFF Generation Functions](#generation)\n",
    " 3. [WFF Evaluation Functions](#evaluation) \n",
    " 4. [Functions to Gather Statistics](#stats)\n",
    " 5. [Model](#model)\n",
    " 6. [Experiment](#experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c8a929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "# built-ins\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import string\n",
    "# packages\n",
    "import numpy as np\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e5010",
   "metadata": {},
   "source": [
    "## 1. Language Preliminaries <a name=\"language\"></a>\n",
    "\n",
    "As discussed in the paper, the language is a first-order language containing connectives, one-place predicates, and constants. It (rather arbitrarily) contains 9 predicates {F, ..., N}. \n",
    "\n",
    "We model an ambiguous technical word using the predicate F. We make the predicate ambiguous between a \"vernacular\" meaning and a \"technical\" meaning. The vernacular meaning is generally broader, hard-coded so that F is true of constants {a, ..., g} on the vernacular meaning but false on the technical meaning. Since technical meanings do often extend beyond vernacular meanings, we make F true of constant z on the technical meaning but not the vernacular. For every other constant, we randomly assign the same truth value on both meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a3d691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define language base\n",
    "connectives = ['~', '&', '>', 'V'] # negation, conjunction, conditional, disjunction\n",
    "names = [i for i in string.ascii_lowercase]\n",
    "unambiguous_predicates = ['G', 'H', 'I', 'J', 'K', 'L', 'M', 'N']\n",
    "ambiguous_predicates = ['F']\n",
    "hidden_predicates = ['X']\n",
    "# F is actually ambiguous between \"vernacular\" F and \"technical\" X.\n",
    "# Manually set truth values of some ambiguous monads\n",
    "true_ambiguous_monads = ['Fa', 'Fb', 'Fc', 'Fd', 'Fe', 'Ff', 'Fg']\n",
    "false_ambiguous_monads = ['Fz']\n",
    "ambiguous_monads = true_ambiguous_monads + false_ambiguous_monads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d77bbdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mult = 6 # odds of dominant interpretation of ambiguous predicate: (pred_mult-1):1\n",
    "# So that F appears with roughly equal frequency as the other predicates\n",
    "predicates = unambiguous_predicates + ambiguous_predicates + hidden_predicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d869a65a",
   "metadata": {},
   "source": [
    "'!' is the start character and ' ' is the stop character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7baa9399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'F', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '~', '&', '>', 'V', ' '] 41\n"
     ]
    }
   ],
   "source": [
    "chars = ['!'] + [i for i in predicates if i not in hidden_predicates] + names + connectives + [' ']\n",
    "vocab_size = len(chars)\n",
    "print(chars, vocab_size)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda x: [[stoi[c] for c in s] for s in x] # encoder: take a string, output a list of integers\n",
    "decode = lambda x: ''.join([itos[i] for i in x]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbf5ab4",
   "metadata": {},
   "source": [
    "## 2. WFF Generation Functions<a name=\"generation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a89a70",
   "metadata": {},
   "source": [
    "generate_monads() constructs an interpretation for the language by randomly assigning each predicate-constant pair a truth value (1/3 probability of true), except for the hard-coded truth values mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95252de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_monads():\n",
    "    all_monads = []\n",
    "    true_monads = []\n",
    "    false_monads = []\n",
    "    selection_odds = []\n",
    "    for pred in unambiguous_predicates:  \n",
    "        for name in names:  \n",
    "            pos = pred+name\n",
    "            if random.choice([0, 0, 1]) == 1:\n",
    "                true_monads += [pos]\n",
    "            else:\n",
    "                false_monads += [pos]\n",
    "            all_monads += [pos]\n",
    "            selection_odds += [pred_mult]\n",
    "    for pred in ambiguous_predicates:    \n",
    "        for name in names:  \n",
    "            pos = pred+name\n",
    "            if pos in true_ambiguous_monads: # hard-coded \"vernacular\" meanings of ambiguous predicates\n",
    "                true_monads += [pos]\n",
    "                all_monads += [pos]\n",
    "                selection_odds += [pred_mult - 1]\n",
    "            elif pos in false_ambiguous_monads: # hard-coded \"vernacular\" meanings of ambiguous predicates\n",
    "                false_monads += [pos]\n",
    "                all_monads += [pos]\n",
    "                selection_odds += [pred_mult - 1]\n",
    "            elif random.choice([0, 0, 1]) == 1:\n",
    "                true_monads += [pos]\n",
    "                all_monads += [pos]\n",
    "                selection_odds += [pred_mult]\n",
    "            else:\n",
    "                false_monads += [pos]\n",
    "                all_monads += [pos]\n",
    "                selection_odds += [pred_mult]\n",
    "    for i in range(len(hidden_predicates)): # hard-coded \"technical\" meanings of ambiguous predicates\n",
    "        hidden = hidden_predicates[i]\n",
    "        ambig = ambiguous_predicates[i]\n",
    "        for name in names:\n",
    "            pos_ambig = ambig+name\n",
    "            pos_hidden = hidden+name\n",
    "            if pos_ambig in true_ambiguous_monads:\n",
    "                false_monads += [pos_hidden]\n",
    "                all_monads += [pos_hidden]\n",
    "                selection_odds += [1]\n",
    "            elif pos_ambig in false_ambiguous_monads:\n",
    "                true_monads += [pos_hidden]\n",
    "                all_monads += [pos_hidden]\n",
    "                selection_odds += [1]\n",
    "                \n",
    "    return all_monads, true_monads, false_monads, selection_odds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c476f516",
   "metadata": {},
   "source": [
    "generate_prefix() randomly generates WFFs with anywhere from 0 to *max_depth* connectives. The generated WFFs may be either true or false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f01110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prefix(max_depth, monads):\n",
    "    depth = 0\n",
    "    monad_idx = 0\n",
    "    count = 1\n",
    "    exp = ''\n",
    "    while depth < max_depth:\n",
    "        prob = 0.04 * (depth + 2)\n",
    "        if np.random.uniform() < prob:\n",
    "            exp += monads[monad_idx]\n",
    "            monad_idx += 1\n",
    "            count -= 1\n",
    "            break\n",
    "        if count == 1: # connective forced\n",
    "            conn = random.choice(connectives)\n",
    "            exp += conn\n",
    "            if conn != '~':\n",
    "                count += 1\n",
    "            depth += 1\n",
    "        else: # randomly add connective or monad\n",
    "            if random.choice([0, 1]) == 0: # connective\n",
    "                conn = random.choice(connectives)\n",
    "                exp += conn\n",
    "                if conn != '~':\n",
    "                    count += 1\n",
    "                depth += 1\n",
    "            else: # monad\n",
    "                exp += monads[monad_idx]\n",
    "                monad_idx += 1\n",
    "                count -= 1\n",
    "    while count > 0:\n",
    "        exp += monads[monad_idx]\n",
    "        monad_idx += 1\n",
    "        count -= 1\n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "222bd4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_true_wffs(n, max_depth):\n",
    "    true_wffs = []\n",
    "    n_monads = max_depth + 1 # max per wff\n",
    "    monad_array_size = n*n_monads\n",
    "    monad_array = np.random.choice(all_monads, size=monad_array_size, p=selection_probs)\n",
    "    monad_idx = 0\n",
    "    while len(true_wffs) < n:\n",
    "        monad_array_slice = monad_array[monad_idx:monad_idx+n_monads]\n",
    "        wff = generate_prefix(max_depth, monad_array_slice)\n",
    "        tv = evaluate_prefix(wff)\n",
    "        \n",
    "        if tv == 1:\n",
    "            wff = wff.replace('X', 'F') # ambiguity\n",
    "            true_wffs.append(wff)\n",
    "            \n",
    "        monad_idx += n_monads\n",
    "        if monad_idx >= monad_array_size:\n",
    "            monad_array = np.random.choice(all_monads, size=monad_array_size, p=selection_probs)\n",
    "            monad_idx = 0\n",
    "\n",
    "    return true_wffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa1463d",
   "metadata": {},
   "source": [
    "## 3. WFF Evaluation Functions<a name=\"evaluation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40bb9d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_wff(expression):\n",
    "    \"\"\"Evaluate whether a string is a WFF\"\"\"\n",
    "    count = 1\n",
    "    empty_predicate = False\n",
    "    expression_len = len(expression)\n",
    "    for i in range(expression_len):\n",
    "        symbol = expression[i]\n",
    "        if symbol in predicates:\n",
    "            if empty_predicate:\n",
    "                return False\n",
    "            temp_predicate = symbol\n",
    "            empty_predicate = True\n",
    "        elif symbol in names:\n",
    "            if not empty_predicate:\n",
    "                return False\n",
    "            else:\n",
    "                empty_predicate = False\n",
    "                temp_exp = temp_predicate + symbol\n",
    "                count -= 1\n",
    "        elif empty_predicate:\n",
    "            return False\n",
    "        elif symbol in [' ', '!']:\n",
    "            return False\n",
    "        elif symbol == '~':\n",
    "            pass\n",
    "        else: # symbol in binary connectives\n",
    "            count += 1\n",
    "        \n",
    "        if count == 0:\n",
    "            if i+1 < expression_len:\n",
    "                return False\n",
    "        \n",
    "    if count == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "633d9f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prefix(expression): # assumes wff\n",
    "    \"\"\"Evaluate the truth value of an unambiguous wff\"\"\"\n",
    "    stack = []\n",
    "    for i in reversed(expression):\n",
    "        if i in names:\n",
    "            temp_name = i\n",
    "        elif i in predicates:\n",
    "            stack.append(tv_dict[i + temp_name])\n",
    "        elif i in connectives:\n",
    "            if i == '~': # one-place connective\n",
    "                tv = stack.pop()\n",
    "                stack.append(1-tv)\n",
    "            else:\n",
    "                tv1 = stack.pop()\n",
    "                tv2 = stack.pop()\n",
    "                if i == '&': # two-place connective\n",
    "                    stack.append(tv1*tv2)\n",
    "                elif i == 'V':\n",
    "                    stack.append(max(tv1, tv2))\n",
    "                elif i == '>':\n",
    "                    if tv1 == 0 or tv2 == 1:\n",
    "                        stack.append(1)\n",
    "                    else:\n",
    "                        stack.append(0)  \n",
    "    assert len(stack) == 1, 'Stack too big'\n",
    "    return stack[0]\n",
    "\n",
    "\n",
    "def evaluate_ambiguous_prefix(expression): # assumes wff\n",
    "    \"\"\"Evaluate the truth value of an ambiguous wff\"\"\"\n",
    "    # check whether we need to evaluate an ambiguous expression\n",
    "    is_ambiguous = False\n",
    "    for monad in ambiguous_monads:\n",
    "        if monad in expression:\n",
    "            is_ambiguous = True\n",
    "            break\n",
    "            \n",
    "    if is_ambiguous:\n",
    "        stacks = [[]]\n",
    "        for i in reversed(expression):\n",
    "            if i in names:\n",
    "                temp_name = i\n",
    "            elif i in predicates:\n",
    "                pos = i + temp_name\n",
    "                if pos in ambiguous_monads:\n",
    "                    stacks_temp = []\n",
    "                    for stack in stacks:\n",
    "                        stacks_temp += [stack + [1], stack + [0]]\n",
    "                    stacks = stacks_temp\n",
    "                else:\n",
    "                    for stack in stacks:\n",
    "                        stack.append(tv_dict[pos])\n",
    "            elif i in connectives:\n",
    "                for stack in stacks:\n",
    "                    if i == '~': # one-place connective\n",
    "                        tv = stack.pop()\n",
    "                        stack.append(1-tv)\n",
    "                    else: # two-place connective\n",
    "                        tv1 = stack.pop()\n",
    "                        tv2 = stack.pop()\n",
    "                        if i == '&':\n",
    "                            stack.append(tv1*tv2)\n",
    "                        elif i == 'V':\n",
    "                            stack.append(max(tv1, tv2))\n",
    "                        elif i == '>':\n",
    "                            if tv1 == 0 or tv2 == 1:\n",
    "                                stack.append(1)\n",
    "                            else:\n",
    "                                stack.append(0) \n",
    "    else: # unambiguous\n",
    "        stacks = [[evaluate_prefix(expression)]]\n",
    "    return stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba659e10",
   "metadata": {},
   "source": [
    "## 4. Functions to Gather Statistics<a name=\"stats\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2012cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ambiguity_stats(tv, n_true_wffs, n_ambiguous_wffs):\n",
    "    \"\"\"helper function\"\"\"\n",
    "    if len(tv) == 1: # unambiguous\n",
    "        n_true_wffs += tv[0][0]\n",
    "    else:\n",
    "        tvs = [i[0] for i in tv]\n",
    "        if tvs[0] == 1:                        \n",
    "            n_true_wffs += 1\n",
    "            if tvs[-1] == 1: # true on both meanings\n",
    "                n_ambiguous_wffs[0] += 1\n",
    "            else:\n",
    "                n_ambiguous_wffs[2] += 1 # true only on 1st meaning                     \n",
    "        elif tvs[-1] == 1: # in this case, tvs[0] == 0\n",
    "            n_true_wffs += 1\n",
    "            n_ambiguous_wffs[3] += 1\n",
    "        elif sum(tvs) == 0: # false on all meaning combinations\n",
    "            n_ambiguous_wffs[1] += 1\n",
    "        else: # true only on mixed meanings\n",
    "            n_true_wffs += 1\n",
    "            n_ambiguous_wffs[4] += 1 \n",
    "            \n",
    "    return n_true_wffs\n",
    "            \n",
    "def true_wff_stats(true_wffs, iters): \n",
    "    \"\"\"Takes ambiguity stats in a sample from a target population.\"\"\"\n",
    "    n_true_wffs = 0 # ambiguous truth value\n",
    "    # true on either meaning, false on all meanings, true only on 1st meaning, true only on 2nd meaning, true on a mix\n",
    "    n_ambiguous_wffs = [0, 0, 0, 0, 0]\n",
    "    for i in range(iters):\n",
    "        gen = true_wffs[i]\n",
    "        tv = evaluate_ambiguous_prefix(gen) # 0/1\n",
    "        n_true_wffs = update_ambiguity_stats(tv, n_true_wffs, n_ambiguous_wffs)\n",
    "         \n",
    "    return n_true_wffs/iters, [i/iters for i in n_ambiguous_wffs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "170dba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generation_stats(m, iters, batch_size=100):\n",
    "    n_wffs, n_true_wffs = 0, 0 \n",
    "    # true on either meaning, false on all meanings, true only on 1st meaning, true only on 2nd meaning, true on a mix\n",
    "    n_ambiguous_wffs = [0, 0, 0, 0, 0]\n",
    "    \n",
    "    context = torch.zeros((batch_size, 1), dtype=torch.long, device=device)\n",
    "    m.eval() # set evaluation mode\n",
    "    for _ in range(iters//batch_size):\n",
    "        for gen in m.generate(context, max_new_tokens=block_size):\n",
    "            gen = decode(gen.tolist()[1:]).strip()\n",
    "            is_wff = check_wff(gen)\n",
    "            if is_wff:\n",
    "                n_wffs += 1\n",
    "                try:\n",
    "                    tv = evaluate_ambiguous_prefix(gen) # 0/1\n",
    "                except: # sometimes generates ' '\n",
    "                    continue               \n",
    "                n_true_wffs = update_ambiguity_stats(tv, n_true_wffs, n_ambiguous_wffs)     \n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    return n_wffs/iters, n_true_wffs/iters, [i/iters for i in n_ambiguous_wffs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981fa0bf",
   "metadata": {},
   "source": [
    "## 5. Model<a name=\"model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0edd6927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91466070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data), (batch_size,))\n",
    "    x = torch.stack([data[i][:-1] for i in ix])\n",
    "    y = torch.stack([data[i][1:] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a579cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39cbe28",
   "metadata": {},
   "source": [
    "## 6. Experiment<a name=\"experiment\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f9e1566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "max_iters = 250000\n",
    "eval_interval = max_iters//25\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 288 # 384\n",
    "n_head = 6\n",
    "n_layer = 2\n",
    "dropout = 0.2\n",
    "\n",
    "stats_iters = 100000 # number of sentences to generate when gathering model stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "223b7a03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 19\n",
      "(1.0, [0.061138, 0.0, 0.031741, 0.005855, 7e-05])\n",
      "Seed 19. Iter 10000: (0.99678, 0.80827, [0.05594, 0.00648, 0.02146, 0.01184, 0.00015])\n",
      "Seed 19. Iter 20000: (0.99846, 0.88683, [0.06085, 0.00431, 0.02363, 0.00905, 0.0001])\n",
      "Seed 19. Iter 30000: (0.99868, 0.9035, [0.06732, 0.00352, 0.02603, 0.00921, 9e-05])\n",
      "Seed 19. Iter 40000: (0.99897, 0.90868, [0.0628, 0.00309, 0.02603, 0.00837, 0.0001])\n",
      "Seed 19. Iter 50000: (0.99909, 0.92669, [0.06997, 0.00242, 0.02872, 0.00892, 0.00011])\n",
      "Seed 19. Iter 60000: (0.99912, 0.9217, [0.06602, 0.00304, 0.02586, 0.0092, 0.00012])\n",
      "Seed 19. Iter 70000: (0.99906, 0.94192, [0.07222, 0.00207, 0.0276, 0.00814, 8e-05])\n",
      "Seed 19. Iter 80000: (0.9993, 0.96835, [0.06285, 0.00108, 0.02682, 0.00598, 5e-05])\n",
      "Seed 19. Iter 90000: (0.99939, 0.97802, [0.06247, 0.0007, 0.02732, 0.005, 6e-05])\n",
      "Seed 19. Iter 100000: (0.99931, 0.9849, [0.06577, 0.00059, 0.02867, 0.00517, 0.00015])\n",
      "Seed 19. Iter 110000: (0.99934, 0.98804, [0.06427, 0.0004, 0.02661, 0.00555, 7e-05])\n",
      "Seed 19. Iter 120000: (0.99933, 0.99196, [0.06809, 0.00028, 0.02747, 0.00412, 5e-05])\n",
      "Seed 19. Iter 130000: (0.99924, 0.9927, [0.06473, 0.0002, 0.02747, 0.00397, 3e-05])\n",
      "Seed 19. Iter 140000: (0.99936, 0.99307, [0.06591, 0.00035, 0.02996, 0.00542, 9e-05])\n",
      "Seed 19. Iter 150000: (0.99945, 0.99575, [0.06922, 0.00024, 0.02951, 0.00443, 4e-05])\n",
      "Seed 19. Iter 160000: (0.99936, 0.99555, [0.06692, 0.00028, 0.03054, 0.00437, 5e-05])\n",
      "Seed 19. Iter 170000: (0.99943, 0.99564, [0.06565, 0.0002, 0.02783, 0.00431, 6e-05])\n",
      "Seed 19. Iter 180000: (0.99948, 0.9964, [0.06519, 0.00013, 0.03226, 0.00434, 0.0001])\n",
      "Seed 19. Iter 190000: (0.9995, 0.99771, [0.06609, 7e-05, 0.03141, 0.00408, 3e-05])\n",
      "Seed 19. Iter 200000: (0.9993, 0.99666, [0.06577, 0.00013, 0.03295, 0.00575, 0.0001])\n",
      "Seed 19. Iter 210000: (0.99942, 0.99711, [0.06371, 0.00012, 0.02619, 0.00492, 4e-05])\n",
      "Seed 19. Iter 220000: (0.99947, 0.99737, [0.06735, 0.00013, 0.03104, 0.0048, 5e-05])\n",
      "Seed 19. Iter 230000: (0.99944, 0.99753, [0.06442, 0.00014, 0.02746, 0.00353, 2e-05])\n",
      "Seed 19. Iter 240000: (0.99949, 0.99794, [0.07166, 9e-05, 0.03382, 0.00551, 0.0001])\n",
      "Seed 19. Iter 249999: (0.99963, 0.99803, [0.06839, 0.00015, 0.03082, 0.00467, 0.00013])\n"
     ]
    }
   ],
   "source": [
    "for seed in range(19, 20):\n",
    "    print('Seed', seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    all_monads, true_monads, false_monads, selection_odds = generate_monads()\n",
    "\n",
    "    tv_dict = {exp: 1 for exp in true_monads}\n",
    "    for exp in false_monads:\n",
    "        tv_dict[exp] = 0\n",
    "\n",
    "    odds_sum = sum(selection_odds)\n",
    "    selection_probs = [odds/odds_sum for odds in selection_odds]\n",
    "    selection_idx = [i for i in range(len(selection_probs))]\n",
    "\n",
    "    true_wffs = []\n",
    "    for i in range(10):\n",
    "        true_wffs += generate_true_wffs(1000000, 3) \n",
    "    max_length = max([len(i) for i in true_wffs])\n",
    "    block_size = max_length # what is the maximum context length for predictions?\n",
    "\n",
    "    seed_true_stats = true_wff_stats(true_wffs, 1000000) \n",
    "    print(seed_true_stats)\n",
    "\n",
    "    true_wffs = ['!' + wff + ' '*(max_length - len(wff)) for wff in true_wffs]\n",
    "    text = true_wffs\n",
    "\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "\n",
    "    model = GPTLanguageModel()\n",
    "    m = model.to(device)\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "    seed_gen_stats = []\n",
    "    for iteration in range(max_iters):\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iteration % eval_interval == 0 or iteration == max_iters - 1:\n",
    "            if iteration > 0:\n",
    "                iteration_stats = (iteration, model_generation_stats(m, stats_iters, batch_size=stats_iters//50))\n",
    "                print('Seed {}. Iter {}: {}'.format(seed, iteration, iteration_stats[1]))\n",
    "                seed_gen_stats.append(iteration_stats)\n",
    "            m.train() # set training mode\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = m(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    stats_dict = {'true': seed_true_stats, 'generated': seed_gen_stats}\n",
    "    \n",
    "    with open('stats/training_stats_{}.pkl'.format(seed), 'wb') as f:\n",
    "        pickle.dump(stats_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
